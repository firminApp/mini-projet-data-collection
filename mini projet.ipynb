{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28c268e",
   "metadata": {},
   "source": [
    "# Projet de Web Scraping - Dakar Auto\n",
    "\n",
    "Ce notebook va scraper et nettoyer les donn√©es de trois cat√©gories sur dakar-auto.com:\n",
    "1. Voitures\n",
    "2. Motos et Scooters\n",
    "3. Location de voitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab27b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "# Ex√©cutez cette cellule une seule fois\n",
    "import sys\n",
    "!{sys.executable} -m pip install requests beautifulsoup4 pandas lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996f1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb992b",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires pour le scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66102f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url: str) -> BeautifulSoup:\n",
    "    \"\"\"R√©cup√®re le contenu HTML d'une page\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'lxml')\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la r√©cup√©ration de {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte en supprimant les espaces superflus\"\"\"\n",
    "    if text:\n",
    "        return re.sub(r'\\s+', ' ', text.strip())\n",
    "    return \"\"\n",
    "\n",
    "def extract_number(text: str) -> str:\n",
    "    \"\"\"Extrait les nombres d'un texte\"\"\"\n",
    "    if text:\n",
    "        numbers = re.findall(r'\\d+', text.replace(' ', ''))\n",
    "        return ''.join(numbers) if numbers else \"\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe32d65",
   "metadata": {},
   "source": [
    "## 1. Scraping des Voitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96259a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_voitures(base_url: str, max_pages: int = 2773) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape les donn√©es des voitures\n",
    "    Variables: marque, ann√©e, prix, adresse, kilom√©trage, boite vitesse, carburant, propri√©taire\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        soup = get_page_content(url)\n",
    "        \n",
    "        if not soup:\n",
    "            break\n",
    "            \n",
    "        articles = soup.find_all('div', class_='listings-cards__list-item')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"Aucun article trouv√© sur la page {page}\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            try:\n",
    "                data = {}\n",
    "                \n",
    "                # Marque (titre de l'annonce)\n",
    "                title_elem = article.find('h2', class_='listing-card__header__title')\n",
    "                if title_elem:\n",
    "                    title_link = title_elem.find('a')\n",
    "                    data['marque'] = clean_text(title_link.get_text()) if title_link else clean_text(title_elem.get_text())\n",
    "                else:\n",
    "                    data['marque'] = \"\"\n",
    "                \n",
    "                # Prix\n",
    "                price_elem = article.find('h3', class_='listing-card__header__price')\n",
    "                data['prix'] = clean_text(price_elem.get_text()) if price_elem else \"\"\n",
    "                \n",
    "                # Adresse (ville + province)\n",
    "                address_parts = []\n",
    "                town_elem = article.find('span', class_='town-suburb')\n",
    "                if town_elem:\n",
    "                    address_parts.append(clean_text(town_elem.get_text()))\n",
    "                province_elem = article.find('span', class_='province')\n",
    "                if province_elem:\n",
    "                    address_parts.append(clean_text(province_elem.get_text()))\n",
    "                data['adresse'] = ' '.join(address_parts)\n",
    "                \n",
    "                # Initialiser les caract√©ristiques\n",
    "                data['ann√©e'] = \"\"\n",
    "                data['kilom√©trage'] = \"\"\n",
    "                data['boite_vitesse'] = \"\"\n",
    "                data['carburant'] = \"\"\n",
    "                \n",
    "                # Extraire l'ann√©e du titre si pr√©sent\n",
    "                if data['marque']:\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', data['marque'])\n",
    "                    if year_match:\n",
    "                        data['ann√©e'] = year_match.group()\n",
    "                \n",
    "                # Caract√©ristiques dans les li de la liste\n",
    "                specs = article.find_all('li', class_='listing-card__attribute')\n",
    "                for spec in specs:\n",
    "                    spec_text = clean_text(spec.get_text()).lower()\n",
    "                    \n",
    "                    # Kilom√©trage (contient \"km\")\n",
    "                    if 'km' in spec_text and 'km' not in data['kilom√©trage']:\n",
    "                        data['kilom√©trage'] = clean_text(spec.get_text())\n",
    "                    \n",
    "                    # Bo√Æte de vitesse\n",
    "                    if 'automatique' in spec_text or 'manuelle' in spec_text:\n",
    "                        data['boite_vitesse'] = clean_text(spec.get_text())\n",
    "                    \n",
    "                    # Carburant\n",
    "                    if any(word in spec_text for word in ['essence', 'diesel', 'hybride', '√©lectrique']):\n",
    "                        data['carburant'] = clean_text(spec.get_text())\n",
    "                \n",
    "                # Propri√©taire (dans le texte \"Par [nom]\")\n",
    "                author_elem = article.find('p', class_='time-author')\n",
    "                if author_elem:\n",
    "                    author_link = author_elem.find('a')\n",
    "                    if author_link:\n",
    "                        author_text = clean_text(author_link.get_text())\n",
    "                        # Enlever \"Par \" du d√©but\n",
    "                        data['propri√©taire'] = author_text.replace('Par ', '').strip()\n",
    "                    else:\n",
    "                        data['propri√©taire'] = clean_text(author_elem.get_text()).replace('Par ', '').strip()\n",
    "                else:\n",
    "                    data['propri√©taire'] = \"\"\n",
    "                \n",
    "                all_data.append(data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        time.sleep(1)  # Pause pour ne pas surcharger le serveur\n",
    "    \n",
    "    print(f\"Total voitures scrap√©es: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a97530",
   "metadata": {},
   "source": [
    "## 2. Scraping des Motos et Scooters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6cc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_motos(base_url: str, max_pages: int = 55) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape les donn√©es des motos et scooters\n",
    "    Variables: marque, ann√©e, prix, adresse, kilom√©trage, propri√©taire\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        soup = get_page_content(url)\n",
    "        \n",
    "        if not soup:\n",
    "            break\n",
    "            \n",
    "        # Trouver tous les articles/annonces\n",
    "        articles = soup.find_all('div', class_='listings-cards__list-item')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"Aucun article trouv√© sur la page {page}\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            try:\n",
    "                data = {}\n",
    "                \n",
    "                # Marque (titre de l'annonce)\n",
    "                title_elem = article.find('h2', class_='listing-card__header__title')\n",
    "                if title_elem:\n",
    "                    title_link = title_elem.find('a')\n",
    "                    data['marque'] = clean_text(title_link.get_text()) if title_link else clean_text(title_elem.get_text())\n",
    "                else:\n",
    "                    data['marque'] = \"\"\n",
    "                \n",
    "                # Prix\n",
    "                price_elem = article.find('h3', class_='listing-card__header__price')\n",
    "                data['prix'] = clean_text(price_elem.get_text()) if price_elem else \"\"\n",
    "                \n",
    "                # Adresse (ville + province)\n",
    "                address_parts = []\n",
    "                town_elem = article.find('span', class_='town-suburb')\n",
    "                if town_elem:\n",
    "                    address_parts.append(clean_text(town_elem.get_text()))\n",
    "                province_elem = article.find('span', class_='province')\n",
    "                if province_elem:\n",
    "                    address_parts.append(clean_text(province_elem.get_text()))\n",
    "                data['adresse'] = ' '.join(address_parts)\n",
    "                \n",
    "                # Initialiser les caract√©ristiques\n",
    "                data['ann√©e'] = \"\"\n",
    "                data['kilom√©trage'] = \"\"\n",
    "                \n",
    "                # Extraire l'ann√©e du titre si pr√©sent\n",
    "                if data['marque']:\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', data['marque'])\n",
    "                    if year_match:\n",
    "                        data['ann√©e'] = year_match.group()\n",
    "                \n",
    "                # Caract√©ristiques dans les li\n",
    "                specs = article.find_all('li', class_='listing-card__attribute')\n",
    "                for spec in specs:\n",
    "                    spec_text = clean_text(spec.get_text()).lower()\n",
    "                    \n",
    "                    # Kilom√©trage\n",
    "                    if 'km' in spec_text and 'km' not in data['kilom√©trage']:\n",
    "                        data['kilom√©trage'] = clean_text(spec.get_text())\n",
    "                \n",
    "                # Propri√©taire\n",
    "                author_elem = article.find('p', class_='time-author')\n",
    "                if author_elem:\n",
    "                    author_link = author_elem.find('a')\n",
    "                    if author_link:\n",
    "                        author_text = clean_text(author_link.get_text())\n",
    "                        data['propri√©taire'] = author_text.replace('Par ', '').strip()\n",
    "                    else:\n",
    "                        data['propri√©taire'] = clean_text(author_elem.get_text()).replace('Par ', '').strip()\n",
    "                else:\n",
    "                    data['propri√©taire'] = \"\"\n",
    "                \n",
    "                all_data.append(data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Total motos scrap√©es: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef1920",
   "metadata": {},
   "source": [
    "## 3. Scraping des Locations de Voitures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e697cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_locations(base_url: str, max_pages: int = 9) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape les donn√©es des locations de voitures\n",
    "    Variables: marque, ann√©e, prix, adresse, propri√©taire\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        soup = get_page_content(url)\n",
    "        \n",
    "        if not soup:\n",
    "            break\n",
    "            \n",
    "        # Trouver tous les articles/annonces\n",
    "        articles = soup.find_all('div', class_='listings-cards__list-item')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"Aucun article trouv√© sur la page {page}\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            try:\n",
    "                data = {}\n",
    "                \n",
    "                # Marque (titre de l'annonce)\n",
    "                title_elem = article.find('h2', class_='listing-card__header__title')\n",
    "                if title_elem:\n",
    "                    title_link = title_elem.find('a')\n",
    "                    data['marque'] = clean_text(title_link.get_text()) if title_link else clean_text(title_elem.get_text())\n",
    "                else:\n",
    "                    data['marque'] = \"\"\n",
    "                \n",
    "                # Prix\n",
    "                price_elem = article.find('h3', class_='listing-card__header__price')\n",
    "                data['prix'] = clean_text(price_elem.get_text()) if price_elem else \"\"\n",
    "                \n",
    "                # Adresse (ville + province)\n",
    "                address_parts = []\n",
    "                town_elem = article.find('span', class_='town-suburb')\n",
    "                if town_elem:\n",
    "                    address_parts.append(clean_text(town_elem.get_text()))\n",
    "                province_elem = article.find('span', class_='province')\n",
    "                if province_elem:\n",
    "                    address_parts.append(clean_text(province_elem.get_text()))\n",
    "                data['adresse'] = ' '.join(address_parts)\n",
    "                \n",
    "                # Ann√©e - extraire du titre\n",
    "                data['ann√©e'] = \"\"\n",
    "                if data['marque']:\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', data['marque'])\n",
    "                    if year_match:\n",
    "                        data['ann√©e'] = year_match.group()\n",
    "                \n",
    "                # Propri√©taire\n",
    "                author_elem = article.find('p', class_='time-author')\n",
    "                if author_elem:\n",
    "                    author_link = author_elem.find('a')\n",
    "                    if author_link:\n",
    "                        author_text = clean_text(author_link.get_text())\n",
    "                        data['propri√©taire'] = author_text.replace('Par ', '').strip()\n",
    "                    else:\n",
    "                        data['propri√©taire'] = clean_text(author_elem.get_text()).replace('Par ', '').strip()\n",
    "                else:\n",
    "                    data['propri√©taire'] = \"\"\n",
    "                \n",
    "                all_data.append(data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Total locations scrap√©es: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd3876",
   "metadata": {},
   "source": [
    "## 4. Fonction de nettoyage des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd72d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie et standardise un DataFrame\"\"\"\n",
    "    \n",
    "    # Supprimer les lignes compl√®tement vides\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # Nettoyer les prix\n",
    "    if 'prix' in df.columns:\n",
    "        df['prix'] = df['prix'].apply(lambda x: extract_number(str(x)) if pd.notna(x) else \"\")\n",
    "        # Convertir en num√©rique si possible\n",
    "        df['prix_numeric'] = pd.to_numeric(df['prix'], errors='coerce')\n",
    "    \n",
    "    # Nettoyer les kilom√©trages\n",
    "    if 'kilom√©trage' in df.columns:\n",
    "        df['kilom√©trage_clean'] = df['kilom√©trage'].apply(lambda x: extract_number(str(x)) if pd.notna(x) else \"\")\n",
    "        df['kilom√©trage_numeric'] = pd.to_numeric(df['kilom√©trage_clean'], errors='coerce')\n",
    "    \n",
    "    # Nettoyer les ann√©es\n",
    "    if 'ann√©e' in df.columns:\n",
    "        df['ann√©e'] = df['ann√©e'].apply(lambda x: extract_number(str(x)) if pd.notna(x) else \"\")\n",
    "        df['ann√©e_numeric'] = pd.to_numeric(df['ann√©e'], errors='coerce')\n",
    "    \n",
    "    # Supprimer les doublons bas√©s sur marque et prix\n",
    "    if 'marque' in df.columns and 'prix' in df.columns:\n",
    "        df = df.drop_duplicates(subset=['marque', 'prix'], keep='first')\n",
    "    \n",
    "    # R√©initialiser l'index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439efc3",
   "metadata": {},
   "source": [
    "## 5. Ex√©cution du scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23998dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCRAPING DES VOITURES\n",
      "============================================================\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Total voitures scrap√©es: 100\n",
      "\n",
      "============================================================\n",
      "SCRAPING DES MOTOS ET SCOOTERS\n",
      "============================================================\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Total motos scrap√©es: 100\n",
      "\n",
      "============================================================\n",
      "SCRAPING DES LOCATIONS\n",
      "============================================================\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Total locations scrap√©es: 100\n",
      "\n",
      "============================================================\n",
      "R√âSUM√â\n",
      "============================================================\n",
      "Voitures scrap√©es: 98\n",
      "Motos scrap√©es: 95\n",
      "Locations scrap√©es: 96\n"
     ]
    }
   ],
   "source": [
    "# URLs √† scraper\n",
    "url_voitures = \"https://dakar-auto.com/senegal/voitures-4\"\n",
    "url_motos = \"https://dakar-auto.com/senegal/motos-and-scooters-3\"\n",
    "url_locations = \"https://dakar-auto.com/senegal/location-de-voitures-19\"\n",
    "\n",
    "# Nombre de pages √† scraper (ajustez selon vos besoins)\n",
    "MAX_PAGES = 5\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SCRAPING DES VOITURES\")\n",
    "print(\"=\"*60)\n",
    "voitures_data = scrape_voitures(url_voitures, max_pages=MAX_PAGES)\n",
    "df_voitures = pd.DataFrame(voitures_data)\n",
    "df_voitures = clean_dataframe(df_voitures)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRAPING DES MOTOS ET SCOOTERS\")\n",
    "print(\"=\"*60)\n",
    "motos_data = scrape_motos(url_motos, max_pages=MAX_PAGES)\n",
    "df_motos = pd.DataFrame(motos_data)\n",
    "df_motos = clean_dataframe(df_motos)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRAPING DES LOCATIONS\")\n",
    "print(\"=\"*60)\n",
    "locations_data = scrape_locations(url_locations, max_pages=MAX_PAGES)\n",
    "df_locations = pd.DataFrame(locations_data)\n",
    "df_locations = clean_dataframe(df_locations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"R√âSUM√â\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Voitures scrap√©es: {len(df_voitures)}\")\n",
    "print(f\"Motos scrap√©es: {len(df_motos)}\")\n",
    "print(f\"Locations scrap√©es: {len(df_locations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031c029",
   "metadata": {},
   "source": [
    "## 6. Affichage et analyse des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68192000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOITURES - Premi√®res lignes\n",
      "============================================================\n",
      "              marque      prix               adresse ann√©e kilom√©trage  \\\n",
      "0  Peugeot 2008 2023   5000000         Fass, Kaolack  2008    20000 km   \n",
      "1    Honda CR-V 2012   4950000           Fann, Dakar  2012   221094 km   \n",
      "2   Ford Fusion 2014   3800000  Yeumbeul Nord, Dakar  2014   155000 km   \n",
      "3   Opel Antara 2007   2100000     Nord Foire, Dakar  2007   171107 km   \n",
      "4   Lexus GX460 2020  38000000    Ouest Foire, Dakar  2020    55493 km   \n",
      "\n",
      "  boite_vitesse carburant   propri√©taire  prix_numeric kilom√©trage_clean  \\\n",
      "0   Automatique   Essence  MOURAD ENNAJY       5000000             20000   \n",
      "1   Automatique   Essence    fafa ndiaye       4950000            221094   \n",
      "2   Automatique   Essence  Cheikh Mback√©       3800000            155000   \n",
      "3   Automatique   Essence    Assane Ndao       2100000            171107   \n",
      "4   Automatique   Essence   Ulrich MI√âR√â      38000000             55493   \n",
      "\n",
      "   kilom√©trage_numeric  ann√©e_numeric  \n",
      "0                20000           2008  \n",
      "1               221094           2012  \n",
      "2               155000           2014  \n",
      "3               171107           2007  \n",
      "4                55493           2020  \n",
      "\n",
      "Shape: (98, 12)\n",
      "Colonnes: ['marque', 'prix', 'adresse', 'ann√©e', 'kilom√©trage', 'boite_vitesse', 'carburant', 'propri√©taire', 'prix_numeric', 'kilom√©trage_clean', 'kilom√©trage_numeric', 'ann√©e_numeric']\n",
      "\n",
      "============================================================\n",
      "MOTOS - Premi√®res lignes\n",
      "============================================================\n",
      "              marque     prix                     adresse ann√©e kilom√©trage  \\\n",
      "0      Honda SH 2010      450           Gu√©diawaye, Dakar  2010      160 km   \n",
      "1     Honda CBF 2007  1500000  Parcelles Assainies, Dakar  2007    11000 km   \n",
      "2      SYM 125S 2023   620000             Rufisque, Dakar  2023     1200 km   \n",
      "3   Yamaha TMax 2023  4300000                  VDN, Dakar  2023        1 km   \n",
      "4  Yamaha X-Max 2025   800000         Sicap Baobab, Dakar  2025      250 km   \n",
      "\n",
      "     propri√©taire  prix_numeric kilom√©trage_clean  kilom√©trage_numeric  \\\n",
      "0  Babacar Diallo           450               160                160.0   \n",
      "1   AMADOU NDIAYE       1500000             11000              11000.0   \n",
      "2     Lamine Ndao        620000              1200               1200.0   \n",
      "3     Rose DIOMPY       4300000                 1                  1.0   \n",
      "4     Rose DIOMPY        800000               250                250.0   \n",
      "\n",
      "   ann√©e_numeric  \n",
      "0           2010  \n",
      "1           2007  \n",
      "2           2023  \n",
      "3           2023  \n",
      "4           2025  \n",
      "\n",
      "Shape: (95, 10)\n",
      "Colonnes: ['marque', 'prix', 'adresse', 'ann√©e', 'kilom√©trage', 'propri√©taire', 'prix_numeric', 'kilom√©trage_clean', 'kilom√©trage_numeric', 'ann√©e_numeric']\n",
      "\n",
      "============================================================\n",
      "LOCATIONS - Premi√®res lignes\n",
      "============================================================\n",
      "                  marque     prix            adresse ann√©e  \\\n",
      "0  Hyundai Santa Fe 2017    35000       Thi√®s, Thi√®s  2017   \n",
      "1   Mitsubishi L200 2018    50000       Thi√®s, Thi√®s  2018   \n",
      "2    Ford ESCAPE-SE 2013    30000  Gu√©diawaye, Dakar  2013   \n",
      "3        Ford scape 2013  4500000  Gu√©diawaye, Dakar  2013   \n",
      "4         Ford Edge 2017    45000  Gu√©diawaye, Dakar  2017   \n",
      "\n",
      "              propri√©taire  prix_numeric  ann√©e_numeric  \n",
      "0  AlfaBusinessGroup BARRY         35000           2017  \n",
      "1  AlfaBusinessGroup BARRY         50000           2018  \n",
      "2             TERANGUA BII         30000           2013  \n",
      "3             TERANGUA BII       4500000           2013  \n",
      "4            Mouhamed Sene         45000           2017  \n",
      "\n",
      "Shape: (96, 7)\n",
      "Colonnes: ['marque', 'prix', 'adresse', 'ann√©e', 'propri√©taire', 'prix_numeric', 'ann√©e_numeric']\n"
     ]
    }
   ],
   "source": [
    "# Afficher les premi√®res lignes de chaque dataset\n",
    "print(\"=\"*60)\n",
    "print(\"VOITURES - Premi√®res lignes\")\n",
    "print(\"=\"*60)\n",
    "print(df_voitures.head())\n",
    "print(f\"\\nShape: {df_voitures.shape}\")\n",
    "print(f\"Colonnes: {list(df_voitures.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOTOS - Premi√®res lignes\")\n",
    "print(\"=\"*60)\n",
    "print(df_motos.head())\n",
    "print(f\"\\nShape: {df_motos.shape}\")\n",
    "print(f\"Colonnes: {list(df_motos.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOCATIONS - Premi√®res lignes\")\n",
    "print(\"=\"*60)\n",
    "print(df_locations.head())\n",
    "print(f\"\\nShape: {df_locations.shape}\")\n",
    "print(f\"Colonnes: {list(df_locations.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643902f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 2: Scraping SANS nettoyage de TOUTES les pages\n",
    "\n",
    "Cette section scrappe toutes les pages disponibles et conserve les donn√©es brutes sans nettoyage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbde83",
   "metadata": {},
   "source": [
    "## Fonction de d√©tection automatique du nombre de pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbcc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_pages(base_url: str) -> int:\n",
    "    \"\"\"D√©tecte automatiquement le nombre total de pages\"\"\"\n",
    "    soup = get_page_content(base_url)\n",
    "    if not soup:\n",
    "        return 1\n",
    "    \n",
    "    try:\n",
    "        paginator = soup.find('nav', class_='paginator')\n",
    "        if paginator:\n",
    "            # Trouver tous les liens de pagination\n",
    "            page_links = paginator.find_all('a', class_='page-link')\n",
    "            max_page = 1\n",
    "            \n",
    "            for link in page_links:\n",
    "                # Extraire le num√©ro de page de l'URL\n",
    "                href = link.get('href', '')\n",
    "                # Chercher tous les param√®tres page= dans l'URL\n",
    "                matches = re.findall(r'page=(\\d+)', href)\n",
    "                if matches:\n",
    "                    # Prendre le dernier param√®tre page= (le vrai num√©ro)\n",
    "                    page_num = int(matches[-1])\n",
    "                    max_page = max(max_page, page_num)\n",
    "            # modification temporaire\n",
    "            return max_page\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la d√©tection du nombre de pages: {e}\")\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3669b0c",
   "metadata": {},
   "source": [
    "## Fonctions de scraping SANS nettoyage - Toutes les pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972e57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_voitures_brut(base_url: str, max_pages: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape les donn√©es brutes des voitures (SANS NETTOYAGE) - TOUTES LES PAGES\n",
    "    Variables: marque, ann√©e, prix, adresse, kilom√©trage, boite vitesse, carburant, propri√©taire\n",
    "    \"\"\"\n",
    "    # D√©tecter automatiquement le nombre de pages si non sp√©cifi√©\n",
    "    if max_pages is None:\n",
    "        print(\"üîç D√©tection du nombre total de pages...\")\n",
    "        max_pages = get_total_pages(base_url)\n",
    "        print(f\"‚úì {max_pages} pages d√©tect√©es\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"üìÑ Scraping page {page}/{max_pages}...\")\n",
    "        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        soup = get_page_content(url)\n",
    "        \n",
    "        if not soup:\n",
    "            print(f\"‚ùå Impossible de r√©cup√©rer la page {page}, arr√™t.\")\n",
    "            break\n",
    "            \n",
    "        # Trouver tous les articles\n",
    "        articles = soup.find_all('div', class_='listings-cards__list-item')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"‚ö†Ô∏è Aucun article trouv√© sur la page {page}, arr√™t.\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            try:\n",
    "                data = {}\n",
    "                \n",
    "                # V1: Marque - BRUT\n",
    "                title_elem = article.find('h2', class_='listing-card__header__title')\n",
    "                if title_elem:\n",
    "                    title_link = title_elem.find('a')\n",
    "                    data['marque'] = title_link.get_text().strip() if title_link else title_elem.get_text().strip()\n",
    "                else:\n",
    "                    data['marque'] = \"\"\n",
    "                \n",
    "                # V2: Ann√©e\n",
    "                data['ann√©e'] = \"\"\n",
    "                if data['marque']:\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', data['marque'])\n",
    "                    if year_match:\n",
    "                        data['ann√©e'] = year_match.group()\n",
    "                \n",
    "                # V3: Prix - BRUT\n",
    "                price_elem = article.find('h3', class_='listing-card__header__price')\n",
    "                data['prix'] = price_elem.get_text().strip() if price_elem else \"\"\n",
    "                \n",
    "                # V4: Adresse - BRUT\n",
    "                address_parts = []\n",
    "                town_elem = article.find('span', class_='town-suburb')\n",
    "                if town_elem:\n",
    "                    address_parts.append(town_elem.get_text().strip())\n",
    "                province_elem = article.find('span', class_='province')\n",
    "                if province_elem:\n",
    "                    address_parts.append(province_elem.get_text().strip())\n",
    "                data['adresse'] = ' '.join(address_parts)\n",
    "                \n",
    "                # V5, V6, V7: Caract√©ristiques - BRUT\n",
    "                data['kilom√©trage'] = \"\"\n",
    "                data['boite_vitesse'] = \"\"\n",
    "                data['carburant'] = \"\"\n",
    "                \n",
    "                specs = article.find_all('li', class_='listing-card__attribute')\n",
    "                for spec in specs:\n",
    "                    spec_text = spec.get_text().strip()\n",
    "                    spec_lower = spec_text.lower()\n",
    "                    \n",
    "                    if 'km' in spec_lower and not data['kilom√©trage']:\n",
    "                        data['kilom√©trage'] = spec_text\n",
    "                    \n",
    "                    if 'automatique' in spec_lower or 'manuelle' in spec_lower:\n",
    "                        data['boite_vitesse'] = spec_text\n",
    "                    \n",
    "                    if any(word in spec_lower for word in ['essence', 'diesel', 'hybride', '√©lectrique']):\n",
    "                        data['carburant'] = spec_text\n",
    "                \n",
    "                # V8: Propri√©taire - BRUT\n",
    "                author_elem = article.find('p', class_='time-author')\n",
    "                if author_elem:\n",
    "                    author_link = author_elem.find('a')\n",
    "                    data['propri√©taire'] = author_link.get_text().strip() if author_link else author_elem.get_text().strip()\n",
    "                else:\n",
    "                    data['propri√©taire'] = \"\"\n",
    "                \n",
    "                all_data.append(data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erreur article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if page < max_pages:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total voitures scrap√©es: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88e72d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_motos_brut(base_url: str, max_pages: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape les donn√©es brutes des motos (SANS NETTOYAGE) - TOUTES LES PAGES\n",
    "    Variables: marque, ann√©e, prix, adresse, kilom√©trage, propri√©taire\n",
    "    \"\"\"\n",
    "    if max_pages is None:\n",
    "        print(\"üîç D√©tection du nombre total de pages...\")\n",
    "        max_pages = get_total_pages(base_url)\n",
    "        print(f\"‚úì {max_pages} pages d√©tect√©es\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"üìÑ Scraping page {page}/{max_pages}...\")\n",
    "        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        soup = get_page_content(url)\n",
    "        \n",
    "        if not soup:\n",
    "            print(f\"‚ùå Impossible de r√©cup√©rer la page {page}, arr√™t.\")\n",
    "            break\n",
    "            \n",
    "        articles = soup.find_all('div', class_='listings-cards__list-item')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"‚ö†Ô∏è Aucun article trouv√© sur la page {page}, arr√™t.\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            try:\n",
    "                data = {}\n",
    "                \n",
    "                # V1: Marque - BRUT\n",
    "                title_elem = article.find('h2', class_='listing-card__header__title')\n",
    "                if title_elem:\n",
    "                    title_link = title_elem.find('a')\n",
    "                    data['marque'] = title_link.get_text().strip() if title_link else title_elem.get_text().strip()\n",
    "                else:\n",
    "                    data['marque'] = \"\"\n",
    "                \n",
    "                # V2: Ann√©e\n",
    "                data['ann√©e'] = \"\"\n",
    "                if data['marque']:\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', data['marque'])\n",
    "                    if year_match:\n",
    "                        data['ann√©e'] = year_match.group()\n",
    "                \n",
    "                # V3: Prix - BRUT\n",
    "                price_elem = article.find('h3', class_='listing-card__header__price')\n",
    "                data['prix'] = price_elem.get_text().strip() if price_elem else \"\"\n",
    "                \n",
    "                # V4: Adresse - BRUT\n",
    "                address_parts = []\n",
    "                town_elem = article.find('span', class_='town-suburb')\n",
    "                if town_elem:\n",
    "                    address_parts.append(town_elem.get_text().strip())\n",
    "                province_elem = article.find('span', class_='province')\n",
    "                if province_elem:\n",
    "                    address_parts.append(province_elem.get_text().strip())\n",
    "                data['adresse'] = ' '.join(address_parts)\n",
    "                \n",
    "                # V5: Kilom√©trage - BRUT\n",
    "                data['kilom√©trage'] = \"\"\n",
    "                specs = article.find_all('li', class_='listing-card__attribute')\n",
    "                for spec in specs:\n",
    "                    spec_text = spec.get_text().strip()\n",
    "                    if 'km' in spec_text.lower() and not data['kilom√©trage']:\n",
    "                        data['kilom√©trage'] = spec_text\n",
    "                        break\n",
    "                \n",
    "                # V6: Propri√©taire - BRUT\n",
    "                author_elem = article.find('p', class_='time-author')\n",
    "                if author_elem:\n",
    "                    author_link = author_elem.find('a')\n",
    "                    data['propri√©taire'] = author_link.get_text().strip() if author_link else author_elem.get_text().strip()\n",
    "                else:\n",
    "                    data['propri√©taire'] = \"\"\n",
    "                \n",
    "                all_data.append(data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erreur article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if page < max_pages:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total motos scrap√©es: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d13f5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_locations_brut(base_url: str, max_pages: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape les donn√©es brutes des locations (SANS NETTOYAGE) - TOUTES LES PAGES\n",
    "    Variables: marque, ann√©e, prix, adresse, propri√©taire\n",
    "    \"\"\"\n",
    "    if max_pages is None:\n",
    "        print(\"üîç D√©tection du nombre total de pages...\")\n",
    "        max_pages = get_total_pages(base_url)\n",
    "        print(f\"‚úì {max_pages} pages d√©tect√©es\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"üìÑ Scraping page {page}/{max_pages}...\")\n",
    "        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        soup = get_page_content(url)\n",
    "        \n",
    "        if not soup:\n",
    "            print(f\"‚ùå Impossible de r√©cup√©rer la page {page}, arr√™t.\")\n",
    "            break\n",
    "            \n",
    "        articles = soup.find_all('div', class_='listings-cards__list-item')\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"‚ö†Ô∏è Aucun article trouv√© sur la page {page}, arr√™t.\")\n",
    "            break\n",
    "            \n",
    "        for article in articles:\n",
    "            try:\n",
    "                data = {}\n",
    "                \n",
    "                # V1: Marque - BRUT\n",
    "                title_elem = article.find('h2', class_='listing-card__header__title')\n",
    "                if title_elem:\n",
    "                    title_link = title_elem.find('a')\n",
    "                    data['marque'] = title_link.get_text().strip() if title_link else title_elem.get_text().strip()\n",
    "                else:\n",
    "                    data['marque'] = \"\"\n",
    "                \n",
    "                # V2: Ann√©e\n",
    "                data['ann√©e'] = \"\"\n",
    "                if data['marque']:\n",
    "                    year_match = re.search(r'\\b(19|20)\\d{2}\\b', data['marque'])\n",
    "                    if year_match:\n",
    "                        data['ann√©e'] = year_match.group()\n",
    "                \n",
    "                # V3: Prix - BRUT\n",
    "                price_elem = article.find('h3', class_='listing-card__header__price')\n",
    "                data['prix'] = price_elem.get_text().strip() if price_elem else \"\"\n",
    "                \n",
    "                # V4: Adresse - BRUT\n",
    "                address_parts = []\n",
    "                town_elem = article.find('span', class_='town-suburb')\n",
    "                if town_elem:\n",
    "                    address_parts.append(town_elem.get_text().strip())\n",
    "                province_elem = article.find('span', class_='province')\n",
    "                if province_elem:\n",
    "                    address_parts.append(province_elem.get_text().strip())\n",
    "                data['adresse'] = ' '.join(address_parts)\n",
    "                \n",
    "                # V5: Propri√©taire - BRUT\n",
    "                author_elem = article.find('p', class_='time-author')\n",
    "                if author_elem:\n",
    "                    author_link = author_elem.find('a')\n",
    "                    data['propri√©taire'] = author_link.get_text().strip() if author_link else author_elem.get_text().strip()\n",
    "                else:\n",
    "                    data['propri√©taire'] = \"\"\n",
    "                \n",
    "                all_data.append(data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erreur article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if page < max_pages:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total locations scrap√©es: {len(all_data)}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab882c3d",
   "metadata": {},
   "source": [
    "## Ex√©cution du scraping SANS nettoyage - TOUTES LES PAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33040440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöó SCRAPING DES VOITURES (TOUTES LES PAGES - SANS NETTOYAGE)\n",
      "================================================================================\n",
      "üîç D√©tection du nombre total de pages...\n",
      "‚úì 5 pages d√©tect√©es\n",
      "\n",
      "üìÑ Scraping page 1/5...\n",
      "üìÑ Scraping page 2/5...\n",
      "üìÑ Scraping page 3/5...\n",
      "üìÑ Scraping page 4/5...\n",
      "üìÑ Scraping page 5/5...\n",
      "\n",
      "‚úÖ Total voitures scrap√©es: 100\n",
      "\n",
      "================================================================================\n",
      "üèçÔ∏è SCRAPING DES MOTOS (TOUTES LES PAGES - SANS NETTOYAGE)\n",
      "================================================================================\n",
      "üîç D√©tection du nombre total de pages...\n",
      "‚úì 5 pages d√©tect√©es\n",
      "\n",
      "üìÑ Scraping page 1/5...\n",
      "üìÑ Scraping page 2/5...\n",
      "üìÑ Scraping page 3/5...\n",
      "üìÑ Scraping page 4/5...\n",
      "üìÑ Scraping page 5/5...\n",
      "\n",
      "‚úÖ Total motos scrap√©es: 100\n",
      "\n",
      "================================================================================\n",
      "üöô SCRAPING DES LOCATIONS (TOUTES LES PAGES - SANS NETTOYAGE)\n",
      "================================================================================\n",
      "üîç D√©tection du nombre total de pages...\n",
      "‚úì 5 pages d√©tect√©es\n",
      "\n",
      "üìÑ Scraping page 1/5...\n",
      "üìÑ Scraping page 2/5...\n",
      "üìÑ Scraping page 3/5...\n",
      "üìÑ Scraping page 4/5...\n",
      "üìÑ Scraping page 5/5...\n",
      "\n",
      "‚úÖ Total locations scrap√©es: 100\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SCRAPING TERMIN√â!\n",
      "================================================================================\n",
      "Voitures: 100 lignes\n",
      "Motos: 100 lignes\n",
      "Locations: 100 lignes\n"
     ]
    }
   ],
   "source": [
    "# URLs\n",
    "URL_VOITURES = \"https://www.dakar-auto.com/senegal/voitures-4\"\n",
    "URL_MOTOS = \"https://www.dakar-auto.com/senegal/motos-and-scooters-3\"\n",
    "URL_LOCATIONS = \"https://www.dakar-auto.com/senegal/location-de-voitures-19\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöó SCRAPING DES VOITURES (TOUTES LES PAGES - SANS NETTOYAGE)\")\n",
    "print(\"=\" * 80)\n",
    "voitures_brutes = scrape_voitures_brut(URL_VOITURES)\n",
    "df_voitures_brut = pd.DataFrame(voitures_brutes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üèçÔ∏è SCRAPING DES MOTOS (TOUTES LES PAGES - SANS NETTOYAGE)\")\n",
    "print(\"=\" * 80)\n",
    "motos_brutes = scrape_motos_brut(URL_MOTOS)\n",
    "df_motos_brut = pd.DataFrame(motos_brutes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöô SCRAPING DES LOCATIONS (TOUTES LES PAGES - SANS NETTOYAGE)\")\n",
    "print(\"=\" * 80)\n",
    "locations_brutes = scrape_locations_brut(URL_LOCATIONS)\n",
    "df_locations_brut = pd.DataFrame(locations_brutes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ SCRAPING TERMIN√â!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Voitures: {len(df_voitures_brut)} lignes\")\n",
    "print(f\"Motos: {len(df_motos_brut)} lignes\")\n",
    "print(f\"Locations: {len(df_locations_brut)} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c62dd",
   "metadata": {},
   "source": [
    "## Aper√ßu des donn√©es brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6596320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó VOITURES (donn√©es brutes):\n",
      "              marque ann√©e              prix               adresse  \\\n",
      "0  Peugeot 2008 2023  2008   5‚ÄØ000‚ÄØ000 F CFA         Fass, Kaolack   \n",
      "1    Honda CR-V 2012  2012   4‚ÄØ950‚ÄØ000 F CFA           Fann, Dakar   \n",
      "2   Ford Fusion 2014  2014   3‚ÄØ800‚ÄØ000 F CFA  Yeumbeul Nord, Dakar   \n",
      "3   Opel Antara 2007  2007   2‚ÄØ100‚ÄØ000 F CFA     Nord Foire, Dakar   \n",
      "4   Lexus GX460 2020  2020  38‚ÄØ000‚ÄØ000 F CFA    Ouest Foire, Dakar   \n",
      "\n",
      "  kilom√©trage boite_vitesse carburant        propri√©taire  \n",
      "0    20000 km   Automatique   Essence   Par MOURAD ENNAJY  \n",
      "1   221094 km   Automatique   Essence     Par fafa ndiaye  \n",
      "2   155000 km   Automatique   Essence  Par Cheikh  Mback√©  \n",
      "3   171107 km   Automatique   Essence    Par Assane  Ndao  \n",
      "4    55493 km   Automatique   Essence    Par Ulrich MI√âR√â  \n",
      "\n",
      "Shape: (100, 8)\n",
      "Valeurs manquantes:\n",
      "marque           0\n",
      "ann√©e            0\n",
      "prix             0\n",
      "adresse          0\n",
      "kilom√©trage      0\n",
      "boite_vitesse    0\n",
      "carburant        0\n",
      "propri√©taire     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üöó VOITURES (donn√©es brutes):\")\n",
    "print(df_voitures_brut.head())\n",
    "print(f\"\\nShape: {df_voitures_brut.shape}\")\n",
    "print(f\"Valeurs manquantes:\\n{df_voitures_brut.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff2a6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèçÔ∏è MOTOS (donn√©es brutes):\n",
      "              marque ann√©e             prix                     adresse  \\\n",
      "0      Honda SH 2010  2010        450 F CFA           Gu√©diawaye, Dakar   \n",
      "1     Honda CBF 2007  2007  1‚ÄØ500‚ÄØ000 F CFA  Parcelles Assainies, Dakar   \n",
      "2      SYM 125S 2023  2023    620‚ÄØ000 F CFA             Rufisque, Dakar   \n",
      "3   Yamaha TMax 2023  2023  4‚ÄØ300‚ÄØ000 F CFA                  VDN, Dakar   \n",
      "4  Yamaha X-Max 2025  2025    800‚ÄØ000 F CFA         Sicap Baobab, Dakar   \n",
      "\n",
      "  kilom√©trage        propri√©taire  \n",
      "0      160 km  Par Babacar Diallo  \n",
      "1    11000 km   Par AMADOU NDIAYE  \n",
      "2     1200 km    Par Lamine  Ndao  \n",
      "3        1 km    Par Rose  DIOMPY  \n",
      "4      250 km    Par Rose  DIOMPY  \n",
      "\n",
      "Shape: (100, 6)\n",
      "Valeurs manquantes:\n",
      "marque          0\n",
      "ann√©e           0\n",
      "prix            0\n",
      "adresse         0\n",
      "kilom√©trage     0\n",
      "propri√©taire    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üèçÔ∏è MOTOS (donn√©es brutes):\")\n",
    "print(df_motos_brut.head())\n",
    "print(f\"\\nShape: {df_motos_brut.shape}\")\n",
    "print(f\"Valeurs manquantes:\\n{df_motos_brut.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3d0fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöô LOCATIONS (donn√©es brutes):\n",
      "                  marque ann√©e             prix            adresse  \\\n",
      "0  Hyundai Santa Fe 2017  2017     35‚ÄØ000 F CFA       Thi√®s, Thi√®s   \n",
      "1   Mitsubishi L200 2018  2018     50‚ÄØ000 F CFA       Thi√®s, Thi√®s   \n",
      "2    Ford ESCAPE-SE 2013  2013     30‚ÄØ000 F CFA  Gu√©diawaye, Dakar   \n",
      "3        Ford scape 2013  2013  4‚ÄØ500‚ÄØ000 F CFA  Gu√©diawaye, Dakar   \n",
      "4         Ford Edge 2017  2017     45‚ÄØ000 F CFA  Gu√©diawaye, Dakar   \n",
      "\n",
      "                  propri√©taire  \n",
      "0  Par AlfaBusinessGroup BARRY  \n",
      "1  Par AlfaBusinessGroup BARRY  \n",
      "2            Par TERANGUA  BII  \n",
      "3            Par TERANGUA  BII  \n",
      "4           Par Mouhamed  Sene  \n",
      "\n",
      "Shape: (100, 5)\n",
      "Valeurs manquantes:\n",
      "marque          0\n",
      "ann√©e           0\n",
      "prix            0\n",
      "adresse         0\n",
      "propri√©taire    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üöô LOCATIONS (donn√©es brutes):\")\n",
    "print(df_locations_brut.head())\n",
    "print(f\"\\nShape: {df_locations_brut.shape}\")\n",
    "print(f\"Valeurs manquantes:\\n{df_locations_brut.isnull().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
